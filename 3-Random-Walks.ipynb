{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multidimensional arrays\n",
    "import numpy as np\n",
    "\n",
    "# inline plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# nicer figures\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.titlesize'] = 16\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['legend.fontsize'] = 12\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return to the origin\n",
    "\n",
    "\n",
    "For a *finite* random walk of (large) length $n$, it is known that the expected number of returns to the origin $T_n$ scales like follows:\n",
    "$$\n",
    "\\left\\langle T_n \\right\\rangle \\sim \\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\sqrt{n} & d=1 \\\\\n",
    "\\log(n) & d=2 \\\\\n",
    "C_d & d\\geq 3\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "<img src=\"attachment:image.png\" width=\"600\">\n",
    "\n",
    "Notice that for $d \\geq 3$, $\\left\\langle T_n \\right\\rangle$ does **not** grow with $n$, which must mean that the walker somehow \"escapes\" and never returns back to the origin. The probability of return to the origin is less than 1! For an infinite-length random walk, indeed the probability of returning to the origin $\\rho$ is seen to be\n",
    "\n",
    "$$\n",
    "\\rho \\sim \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & d=1 \\\\\n",
    "1 & d=2 \\\\\n",
    "<1 & d\\geq 3\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "The *intuitive* explanation of this amazing fact is that, as the dimension $d$ grows, there are \"more directions available\", and so more chances for the walker to \"get lost\" and never return to the origin. There is of course a formal proof as well, but today we will do a **computational verification** of these facts, which is no substitute for a formal proof but is often all we can do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Random Walks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "Write a function that generates a random walk of given length in $d$ dimensions. Your random walker should move as follows:\n",
    "\n",
    "+ At each time-step, the walker moves only in one direction.\n",
    "+ At each time-step, the walker moves only by -1 or +1\n",
    "\n",
    "Your function should return a numpy array of shape (`length`, `dim`). Example:\n",
    "```python\n",
    ">>> # create a RW of length 10 in dimension 3\n",
    ">>> traj = get_traj(length=10, dim=3)\n",
    ">>> # check that the output has the right shape\n",
    ">>> traj.shape\n",
    "(10, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj(length=100, dim=2):\n",
    "    \"\"\"Generate a RW in d dimensions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    length: int\n",
    "        Length of the RW.\n",
    "    dim: int\n",
    "        Dimension of the RW\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    traj : np.ndarray, (length, dim)\n",
    "        The positions of the RW.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    At each time-step, the walker moves in only one direction.\n",
    "    At each time-step, the walker moves by -1 or +1\n",
    "    \"\"\"\n",
    "    #genero vettore che conterrà la traiettoria - vettore è vuoto\n",
    "    #vettore ha length + 1 perchè la posizione di partenza è il primo elemento del vettore\n",
    "    #così facendo per giungere all'ultima casella ho fatto il numero di step richiesto\n",
    "    traj  = np.zeros(shape = (length + 1, dim))\n",
    "    \n",
    "    for t in range(1, length + 1):\n",
    "        #copio contenuto della colonna precedente in modo tale da evolvere sulla posizione dove ho il corpo\n",
    "        traj[t, :] = traj[t-1, : ]\n",
    "        traj[t, np.random.randint(dim,size = 1)] += np.random.choice([-1,1])\n",
    "        \n",
    "    return traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification\n",
    "To make sure that your function works correctly, execute the following cell. Notice the use of `assert` statements: execution should fail if something goes wrong. If everything is fine, nothing should happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic checks for your RW generator\n",
    "for dim in range(1, 5):\n",
    "    for length in [10, 100, 200, 500]:\n",
    "        traj = get_traj(length = length, dim = dim)\n",
    "        # make sure traj has the right shape\n",
    "        assert traj.shape == (length + 1, dim)\n",
    "        # make sure all steps are -1 or 1 in only one direction\n",
    "        assert np.all(np.sum(np.diff(traj, axis=0) != 0, axis=1) == np.ones(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "Plot a random walk of length $10^4$ for $d=1$ (time in x-axis, position in y-axis) and $d=2$ (x,y components in x,y-axis). Remember to use **axis labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# it is better if you use one cell to generate the random walks, and a second cell to plot them\n",
    "RW_1d = get_traj(10 ** 4, 1)\n",
    "RW_2d = get_traj(10 ** 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We generate a figure with two subplots, called axis in matplotlib.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "#creo valori del tempo per il cammino unidimensionale\n",
    "t = np.linspace(0, 10 ** 4, 10 **4 + 1)\n",
    "ax1.plot(\n",
    "        t, RW_1d,\n",
    "        color = \"blue\",\n",
    "        lw = 2,\n",
    "        label = \"Two dimensional random walk\"\n",
    "    )\n",
    "\n",
    "#Titoli assi per il primo grafico\n",
    "ax1.set_xlabel(r\"$Tempo$\")\n",
    "ax1.set_ylabel(r\"$Posizione$\")\n",
    "#Titolo grafico per il primo grafico\n",
    "ax1.set_title(f\"Random Walk 1D\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Riempio il secondo grafico\n",
    "ax2.plot(\n",
    "        RW_2d[:, 0], RW_2d[:, 1],\n",
    "        color = \"red\",\n",
    "        lw = 2,\n",
    "    )\n",
    "\n",
    "#Titoli assi per il secondo grafico\n",
    "ax2.set_xlabel(r\"$X$\")\n",
    "ax2.set_ylabel(r\"$Y$\")\n",
    "#Titolo grafico per il secondo grafico\n",
    "ax2.set_title(f\"Random Walk 2D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the number of returns to the origin\n",
    "Since we are interested in how **the expected number of returns to the origin** scales with the RW length, we don't need to store the whole trajectory of each simulation (we will be performing many simulations!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "Write a function that generates a RW of given length and dimension (calling `get_traj`), and returns the number of times it returned to the origin. To count the number of returns to the origin, you might need to use the following functions:\n",
    "```python\n",
    "np.all()\n",
    "np.zeros()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_returns(length, dim):\n",
    "    #Creo random walk di una certa lunghezza\n",
    "    traj = get_traj(length, dim)\n",
    "    \n",
    "    #printing per test\n",
    "    #print(traj)\n",
    "    \n",
    "    #Creo contatore per confronto\n",
    "    num_returns_to_origin = 0\n",
    "    \n",
    "    # count how many times it goes through the origin\n",
    "    for t in range(1, length + 1):\n",
    "        if np.all(traj[t, :] == 0):\n",
    "            num_returns_to_origin += 1\n",
    "    \n",
    "    return num_returns_to_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test metodo\n",
    "print(get_num_returns(10, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4\n",
    "Write a function that computes the expected number of returns to the origin for a given length and dimension. Your function will call `get_num_returns()`, and should have an additional parameter that sets the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_num_returns(length, dim, num_trajs=200):\n",
    "    #Pongo a zero la variabile di appoggio\n",
    "    avarage_num_returns = 0\n",
    "    \n",
    "    for i in range(0, num_trajs):\n",
    "        avarage_num_returns = avarage_num_returns * i/(i+1) + get_num_returns(length, dim)/(i+1)\n",
    "    \n",
    "    print(f\"Trovata media - lunghezza $RW$ = {length}\")\n",
    "    \n",
    "    return avarage_num_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test metodo\n",
    "print(get_average_num_returns(10, 2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with analytical results\n",
    "We are now ready to compare our analytical results with numerical simulations! We want to plot the expected number of returns to the origin as a function of the RW length. To do this, it is useful to first define an array of RW lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define range of RW lengths\n",
    "length_min = 10\n",
    "length_max = 100000\n",
    "# generate points logarithmically spaces\n",
    "# and convert them to integers\n",
    "length_array = np.array([\n",
    "    int(x)\n",
    "    for x in np.geomspace(length_min, length_max, num=20)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(tip: if your RW generating function is not very efficient, you might want to decrease `length_min`)  \n",
    "\n",
    "Executing the following cell will run all simulations for $d=1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Simulazione RW_1D\n",
    "dim=1\n",
    "num_returns_array = np.array([\n",
    "    get_average_num_returns(length=length, dim=dim, num_trajs = 20)\n",
    "    for length in length_array\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.5\n",
    "Plot the average number of returns to the origin of a 1D RW as a function of the RW length, together with the expected theoretical result. Do your results verify the $n^{1/2}$ scaling? **Tip** Use double-logarithmic scales in your plot. Remember to include label axis, and a legend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "\n",
    "#RISULTATI TEORICI\n",
    "#Determino quanto valga radice di n nei vari casi\n",
    "y_teo = np.sqrt(length_array)\n",
    "ax.loglog( \n",
    "    length_array, y_teo,\n",
    "    color = \"blue\",\n",
    "    lw = 2,\n",
    "    label=\"Theoretical\")\n",
    "\n",
    "# plot numerical result\n",
    "ax.loglog(\n",
    "    length_array, num_returns_array,\n",
    "    color = \"red\",\n",
    "    lw = 2,\n",
    "    label=\"Numerical results\")\n",
    "\n",
    "\n",
    "# add axis labels\n",
    "ax.set_xlabel(r\"RW length\")\n",
    "ax.set_ylabel(r\"Return to zero\")\n",
    "# add a legend\n",
    "ax.legend(loc=(0.95, 0.7), frameon=False)\n",
    "# add a title (e.g. that says what dimension we used)\n",
    "ax.set_title(f\"Numerical vs Theoretical results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.6\n",
    "Plot the average number of returns to the origin of a 2D RW as a function of the RW length. Do your results verify the $log(n)$ scaling? What are the best axis scales to use in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulazione RW_2D\n",
    "dim=2\n",
    "num_returns_array = np.array([\n",
    "    get_average_num_returns(length=length, dim=dim, num_trajs = 10)\n",
    "    for length in length_array\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "\n",
    "#RISULTATI TEORICI\n",
    "#Determino quanto valga logaritmo di n nei vari casi\n",
    "y_teo = np.log(length_array)\n",
    "ax.loglog( \n",
    "    length_array, y_teo,\n",
    "    color = \"blue\",\n",
    "    lw = 2,\n",
    "    label=\"Theoretical\")\n",
    "\n",
    "# plot numerical result\n",
    "ax.loglog(\n",
    "    length_array, num_returns_array,\n",
    "    color = \"red\",\n",
    "    lw = 2,\n",
    "    label=\"Numerical results\")\n",
    "\n",
    "\n",
    "# add axis labels\n",
    "ax.set_xlabel(r\"RW length\")\n",
    "ax.set_ylabel(r\"Return to zero\")\n",
    "# add a legend\n",
    "ax.legend(loc=(0.95, 0.7), frameon=False)\n",
    "# add a title (e.g. that says what dimension we used)\n",
    "ax.set_title(f\"Numerical vs Theoretical results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.7\n",
    "Show numerically that, for $d=3$ and $d=4$, the expected number of returns to the origin is **constant**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulazione per d = 3\n",
    "dim=3\n",
    "num_returns_array_3D = np.array([\n",
    "    get_average_num_returns(length=length, dim=dim, num_trajs = 10)\n",
    "    for length in length_array\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulazione per d = 4\n",
    "dim=4\n",
    "num_returns_array_4D = np.array([\n",
    "    get_average_num_returns(length=length, dim=dim, num_trajs = 10)\n",
    "    for length in length_array\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot con expected number of returns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "#creo valori del tempo per il cammino unidimensionale\n",
    "ax1.plot(\n",
    "        length_array, num_returns_array_3D,\n",
    "        color = \"blue\",\n",
    "        lw = 2,\n",
    "        label = \"Two dimensional random walk\"\n",
    "    )\n",
    "\n",
    "#Titoli assi per il primo grafico\n",
    "ax1.set_xlabel(r\"Lunghezza cammino\")\n",
    "ax1.set_ylabel(r\"Numero passaggi in 0\")\n",
    "#Titolo grafico per il primo grafico\n",
    "ax1.set_title(f\"Random Walk 3D\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Riempio il secondo grafico\n",
    "ax2.plot(\n",
    "        length_array, num_returns_array_4D,\n",
    "        color = \"red\",\n",
    "        lw = 2,\n",
    "    )\n",
    "\n",
    "#Titoli assi per il secondo grafico\n",
    "ax2.set_xlabel(r\"$Lunghezza cammino$\")\n",
    "ax2.set_ylabel(r\"$Numero passaggi in 0$\")\n",
    "#Titolo grafico per il secondo grafico\n",
    "ax2.set_title(f\"Random Walk 4D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Avoiding Walks\n",
    "Self-avoiding walks (SAW) are simply random walks in a regular lattice with the additional constraint that no point can be visited more than once. That is, SAWs cannot intersect themselves. The most well-known application of SAW is to model linear polymers, where obviously two monomers cannot occupy the same space (excluded volume effect).\n",
    "\n",
    "\n",
    "You can read more about self-avoiding walks in this nice introduction by Gordon Slade:\n",
    "\n",
    "[Self-Avoiding Walks, by Gordon Slade](https://www.math.ubc.ca/~slade/intelligencer.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Self-Avoiding Walks\n",
    "Generating a SAW is not trivial. If you try to generate a SAW stochastically, that is, one step at a time, you will miserably fail: your walker might get into traps (configurations with no allowed movements), and if it does you will have to discard your simulation. It turns out you will have to discard your simulation *really* often, so that for large lengths, you will basically never find a valid path. In addition, the paths you will find for short lengths will not come up with the right probabilities. Bear in mind that we want to **uniformly sample** the set of SAW of given length $n$, SAW($n$). That is, we want that all paths from SAW($n$) are generated with the same probability.\n",
    "\n",
    "The solution is to use a Monte Carlo algorithm that, given one element $\\alpha \\in \\text{SAW}(n)$, generates a new one $\\beta \\in \\text{SAW}(n)$ with some probability $P_{\\alpha \\beta}$. If in addition our algorithm satisfies **detailed balance** and is **ergodic**, then we known that it will converge to the equilibrium distribution (the uniform distribution in our case).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pivot algorithm\n",
    "We will implement the pivot algorithm, which is simple, effective, and satisfies detailed balance and ergodicity. You can read about the details of the pivot algorithm here:\n",
    "\n",
    "[The Pivot Algorithm: A Highly Efficient Monte Carlo Method for the Self-Avoiding Walk](https://link.springer.com/article/10.1007/BF01022990)\n",
    "\n",
    "(tip: if you're at home, **do not** use tools such as sci-hub to download the paper).\n",
    "\n",
    "Given a self-avoiding walk of length $n$, the pivot algorithm generates the next walk $\\beta \\in \\text{SAW}(n)$ as follows:\n",
    "\n",
    "1. **Choose a point of $\\alpha$ at random**, splitting the path in two bits: the head (from the origin to the chosen point) and the tail (from the chosen point to the end of the path). Notice that both the head and the tail are SAWs.\n",
    "2. **Apply a transformation to the tail**, leaving the head intact. The transformation must be an orthoganl transformation that leaves the regular lattice intact (so, either a reflection or a $90º, 180º$ or $270º$ rotation). For simplicity, we will use only **rotations** (read the paper to see why this is ok).\n",
    "3. **Check if the new path is self-avoiding**. If so, return it. Otherwise, return the original path.\n",
    "\n",
    "Iterating these steps one obtains a **Markov** chain of SAWs: $\\alpha_1 \\to \\alpha_2 \\to \\dots \\to \\alpha_M $. Notice that $\\alpha_i$ are not uncorrelated, but because the algorithm satisfies detailed balance and is ergodic, we know that it approaches the equilibrium distribution. This means that we can use our Markov chain to compute **expected values** as long as it is long enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the pivot step in 2D\n",
    "To implement the **pivot algorithm** in 2D, we will write one function that does steps 1 and 2, and another function that does step 3. We will also need a function to generate standard 2D random walks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.8\n",
    "Write a function `get_traj` that generates a 2D random walk of given length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj(length):\n",
    "    \n",
    "    #Vettore di traiettoria\n",
    "    traj = np.zeros(shape = (length + 1, 2))\n",
    "    \n",
    "    for t in range (1, length +1):\n",
    "        traj[t, :] = traj[t-1, : ]\n",
    "        traj[t, np.random.randint(2,size = 1)] += np.random.choice([-1,1])\n",
    "             \n",
    "    return traj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test metodo\n",
    "print(get_traj(length = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.9\n",
    "Write a function `pivot_traj` that, given a 2D random walk, performs steps 1 and 2 of the pivot algorithm. Some useful functions:\n",
    "```python\n",
    "np.random.randint()\n",
    "np.concatenate()\n",
    "```\n",
    "You might also want to multiply matrices using the `@` operator:\n",
    "```python\n",
    ">>> a = np.array([[1, 2], [3, 4], [5, 6]])\n",
    ">>> b = np.array([1, 1])\n",
    ">>> a @ b\n",
    "array([ 3,  7, 11])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_traj(traj):\n",
    "    \"\"\"Apply a random rotation to part of a RW.\"\"\"\n",
    "    \n",
    "    # choose the pivot\n",
    "    pivot_index = np.random.randint(0, traj.shape[0])\n",
    "    pivot_point = traj[pivot_index, : ]\n",
    "\n",
    "    # split head and tail - punto pivot lo metto nella testa del percorso rimanente\n",
    "    traj_head = traj[0 : pivot_index, : ]\n",
    "    traj_tail = traj[pivot_index: , :]\n",
    "    \n",
    "    # define the rotation matrices\n",
    "    symmetries = [\n",
    "        # 90 deg rotation\n",
    "        np.array([[0,-1], [1,0]]),\n",
    "        # 180 deg rotation\n",
    "        np.array([[-1,0], [0,-1]]),\n",
    "        # 270 deg rotation\n",
    "        np.array([[0,1], [-1,0]]),\n",
    "    ]\n",
    "    \n",
    "    # choose one rotation at random\n",
    "    c = np.random.randint(0,3)\n",
    "    symmetry = symmetries[c]\n",
    "\n",
    "    #apply the transformation to the tail\n",
    "    appo = traj_tail - pivot_point\n",
    "    \n",
    "    new_tail=np.array([\n",
    "        symmetry @ (t-pivot_point) + pivot_point\n",
    "        for t in traj_tail\n",
    "    ])\n",
    "    \n",
    "    #new_tail= symmetry @ (traj_tail - pivot_point) + pivot_point\n",
    "    \n",
    "    # join the old head with the new tail\n",
    "    new_traj = np.concatenate((traj_head, new_tail))\n",
    "    \n",
    "    return new_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test metodo\n",
    "traj = np.array([[0,0], [0,1], [0,2]])\n",
    "print(pivot_traj(traj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.10\n",
    "Write a function that counts the number of self-intersections of a RW. Notice that SAWs have 0 self intersections, so that will solve step 3 of the pivot algorithm, but will also be useful to generate the initial condition. One way of approaching this exercise is to count how many *different* points the path visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_self_intersections(traj):\n",
    "    \"\"\"Count the number of self-intersections of a RW\"\"\"\n",
    "    counter = 0\n",
    "    for t in range (0, traj.shape[0] - 1):\n",
    "        for w in range (t+1, traj.shape[0]):\n",
    "            if np.all(traj[t, :] == traj[w, :]):\n",
    "                counter += 1\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.11\n",
    "Verify that your `count_self_intersecitons` function works properly by using short trajectories for which you know the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test metodo precedente\n",
    "traj = np.array([[0, 0], [1,0], [1, 1], [1, 0], [1, -1], [0, -1], [0, 0]])\n",
    "num = count_self_intersections(traj)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the initial condition\n",
    "You might have noticed that the pivot algorithm requires an element of SAW($n$) as starting condition, to then generate a Markov chain easily. But how do you get this first element? We will use the following strategy:\n",
    "1. Generate a standard 2D random walk, and count the number of self intersections.\n",
    "2. Apply the pivot transformation to get a new 2D random walk, and count the number of self-intersections-\n",
    "3. If the number of self-intersections has decreased or not changed, keep the new path. Else, keep the old one.\n",
    "4. Go to 2, till the number of self-intersections is 0.\n",
    "\n",
    "### Exercise 3.12\n",
    "Write a function `get_first_SAW` that generates a SAW of given length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_SAW(length, max_tries=1000000):\n",
    "    #Genero il random walk richiesto\n",
    "    traj = get_traj(length = length)\n",
    "    #Valuto quante auto-intersezioni esso abbia\n",
    "    num = count_self_intersections(traj)\n",
    "    num1 = 0\n",
    "    \n",
    "    #Indice per ciclo while\n",
    "    i = 0\n",
    "    \n",
    "    while(num != 0 and i < max_tries):\n",
    "        \n",
    "        #Creo la nuova traiettoria\n",
    "        traj1 = traj\n",
    "        traj = pivot_traj(traj)\n",
    "        \n",
    "        \n",
    "        #Calcolo nuovo numero di auto-intersezioni\n",
    "        num1 = count_self_intersections(traj)\n",
    "        \n",
    "        #Caso in cui num1 > num\n",
    "        if(num1 > num):\n",
    "            traj = traj1\n",
    "            i += 1\n",
    "        \n",
    "        #Devo cambiare il valore di num\n",
    "        else:\n",
    "            num = num1\n",
    "    \n",
    "    if(i < max_tries):\n",
    "        print(\"Trovato SAW\")\n",
    "        return traj\n",
    "\n",
    "    else:\n",
    "        print(\"Non è stato possibile ottenere un SAW\")\n",
    "        return traj\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Test metodo\n",
    "test = get_first_SAW(length = 10, max_tries = 1000)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.13\n",
    "Generate and plot some 2D SAWs of different lengths. Be carefull, raise the length slowly! You can measure how long a cell takes executing using the `%%time` magic at the top of a cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, 10):\n",
    "    \n",
    "    #lunghezza random walk\n",
    "    length = i*10 + 10\n",
    "    #genero SAW\n",
    "    traj_SAW = get_first_SAW(length = length)\n",
    "    \n",
    "    #creo il grafico\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "    # plot SAW\n",
    "    ax.plot(\n",
    "        traj_SAW[ : , 0], traj_SAW[ : , 1], \n",
    "        color = \"red\",\n",
    "        lw = 2,\n",
    "        label = \"Theoretical\"\n",
    "    )\n",
    "    \n",
    "    # add labels for axes\n",
    "    ax.set_xlabel(r\"$x$\")\n",
    "    ax.set_ylabel(r\"Density, $f(x)$\")\n",
    "    \n",
    "    \n",
    "    # add legend\n",
    "    ax.legend(loc=(0.95, 0.7), frameon=False)\n",
    "    \n",
    "    # add title\n",
    "    ax.set_title(f\"SAW per $lunghezza$ ={length}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.14\n",
    "Write a function `get_next_SAW` that, given a SAW, generates another SAW using the pivot algorithm. Your function should check that the input RW is really a SAW. Remember the steps:\n",
    "\n",
    "1. Apply the pivot transformation\n",
    "2. Check if the new path is self-avoiding. **If so, return it. Otherwise, return the original path.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_SAW(traj):\n",
    "    # make sure input traj is SAW\n",
    "    assert count_self_intersections(traj) == 0\n",
    "    \n",
    "    # pivot step\n",
    "    proposed_traj = pivot_traj(traj)\n",
    "    \n",
    "    # count intersections\n",
    "    num_intersections = count_self_intersections(proposed_traj)\n",
    "    \n",
    "    # if it's a SAW\n",
    "    if (num_intersections == 0):\n",
    "        return proposed_traj\n",
    "    \n",
    "    # if not\n",
    "    else:\n",
    "        return traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Displacement in SAW\n",
    "A quantity of interest in RWs is the mean squared displacement, which is simply the (squared) distance between the endpoints of the walk. Usually, one writes\n",
    "\n",
    "$$\n",
    "\\left\\langle X(n)^2 \\right\\rangle \\sim n^{2 \\nu}\n",
    "$$\n",
    "\n",
    "As you know, for a standard RW of $n$ steps, the mean-squared displacement scales like $n$, so $\\nu=1/2$. However, the exponent for SAW is **different**! Althought it has not been formally proven (still), it is believed that the exponent for SAW is $\\nu=3/4$. That is, for a self-avoiding random walk, the mean squared displacement scales as $n^{3/2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.15\n",
    "Explain why it makes sense that the mean-squared displacement exponent of SAW is **greater** than that of standard RW. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "La differenza fra un SAW ed un RW risiede nel fatto che, per la prima tipologia, ciascuna posizione spaziale è ottenibile una sola volta. Per un Random Walk il valore medio di ogni coordinata è identicamente nullo, questo perchè ogni possibile spostamento è equiprobabile. Nel caso dei SAW questo non accade, poichè la struttura precedente del cammino rende non equamente probabili tutte le possibili future configurazioni. In particolare si verranno a creare delle \"regioni vietate\" in cui non è possibile la propagazione del SAW, andando a rompere la condizione di isotropia spaziale. All'aumentare del numero di step costituente un particolare cammino, questa differenza si farà via via più sensibile, infatti potrebbe capitare che i SAW inizino a sviluppare strutture tali per cui \"si intrappolano\" da soli. All'aumentare di N chiaramente queste strutture tendono a scremare i possibili SAW e a favorire l'allontanamento dalla regione di inizio, determinando un maggior valore del Mean Squared Displacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.16\n",
    "Verify numerically the scaling of the mean-squared displacement of SAW. Notice that you don't need to store all the SAWs, just the endpoints. You could follow this scheme:\n",
    "\n",
    "1. Generate a first SAW with your `get_first_SAW()` function\n",
    "2. Generate the next SAW using your `get_next_SAW()` function, and store the endpoint.\n",
    "3. Iterate step 2 for as many steps as required\n",
    "4. Compute the average mean-squared displacement of the stored endpoints\n",
    "\n",
    "Then repeating steps 1-4 for different lengths, and plot the results in double-logarithmic axis. Compare your results with the theoretical exponent. Do they agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define range of RW lengths\n",
    "length_min = 10\n",
    "length_max = 100\n",
    "num = 5\n",
    "# generate points logarithmically spaces\n",
    "# and convert them to integers\n",
    "length_array = np.array([\n",
    "    int(x)\n",
    "    for x in np.geomspace(length_min, length_max, num = num)\n",
    "])\n",
    "\n",
    "print(length_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genero i vari SAW\n",
    "n_iterazioni = 10\n",
    "distance = np.zeros(shape = (num, 1))\n",
    "\n",
    "\n",
    "#Calcolo i valori della diffusione\n",
    "for i in range(0, num):\n",
    "    #Genero il primo cammino SAW\n",
    "    traj = get_first_SAW(length = length_array[i])\n",
    "    #Valuto la prima distanza\n",
    "    distance[i, 0] = traj[length_array[i]-1, 0] ** 2 + traj[length_array[i]-1, 1] ** 2\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Itero ragionamento\n",
    "    for t in range(1, n_iterazioni):\n",
    "        #Evolvo il mio SAW\n",
    "        traj1 = get_next_SAW(traj)\n",
    "        traj = traj1\n",
    "        \n",
    "        #calcolo la distanza media\n",
    "        distance[i, 0] = distance[i, 0] * t/(t+1) + (traj[length_array[i]-1, 0] ** 2 + traj[length_array[i]-1, 1] ** 2)/(t+1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo output grafico\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "\n",
    "#Calcolo risultato teorico\n",
    "y_teo = length_array ** (3/2)\n",
    "ax.loglog( \n",
    "    length_array, y_teo,\n",
    "    color = \"blue\",\n",
    "    lw = 2,\n",
    "    label=\"Theoretical\")\n",
    "\n",
    "\n",
    "ax.loglog(\n",
    "    length_array, distance,\n",
    "    color = \"red\",\n",
    "    lw = 2,\n",
    "    label=\"Numerical results\"\n",
    ")\n",
    "\n",
    "# add axis labels\n",
    "ax.set_xlabel(r\"RW length\")\n",
    "ax.set_ylabel(r\"Return to zero\")\n",
    "# add a legend\n",
    "ax.legend(loc=(0.95, 0.7), frameon=False)\n",
    "# add a title (e.g. that says what dimension we used)\n",
    "ax.set_title(f\"Numerical vs Theoretical results\")\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
